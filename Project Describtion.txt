Hi , My name is srija sinha, I have completed my btech from computer science.I have total experience of 3 years in IT sector.I have worked on multiple technologies - Python(Django Framework),Tibco, and currently working in PowerBi(with ADF -Azure Data Factory Integration).My current job role is an Anaylst.

Project Explation- (IPRCTO ) - PowerBI

1)Intro - I have worked for health Insurance company,this project is aimed to enhance data reporting and visualization process for health insurance company.

2)Problem - The company faced  in amaylayzing large data set related to clims,policyholder,providers,network,which leads to delay in decision making.

3)Role - My role is on Data cleaning and Reporting side, so working as PowerBi Developer.

4)Contribution - As a developer first create end user document for all the things which we will be showing on dashboard. Then get raw data from source , then pull in any specefic ETL tool like SSIS , or Rdbms. After that quering only required field with basic transformation and finally pulling it in power bi wherewe are doing most of cleaning and transformations ,do Data Modeling and analyzing it with the help of diffrent charts and visuals.

5)Timeline - The project started 8 months ago and I am still working on this.

6)Outcome - With the help of KPI 's shown on dashboard , we are able to manage the supply chain , showing the end users demand of diffrent products.

7)Chalenges faced during project - Incomplte data,ensuring data security.

Project Description - Microsoft Azure with Power Bi (Integration):

1)Intro - Molina health care company uses ADF to ingest claims data from multiple sources accross the country and store it in Azure Databricks.Power Bi pulls the data from Azure databricks, allowing the company to create a real time dashboard, and and performance.

2)Use Azure Data Factory for Data ingestion and Transformation -
 A)Data ingestion - ADF can pull the data from various sources like Databases, Cloud storage.
 B)Data Tranformation - We can clean, aggregrate data in ADF sql databases.

3)Store Transformed Data in Azure Data services- we can use to load data in Azure SQL databse.

4)connect PowerBI to Azure Data Sources- we can directly connect to Azure data sources where powerBi can easily handle it.

5)Visualization with PowerBi-
A) Data Modeling- we can conect to data source in PowerBi prepared by ADF ,model the data,create relationship and build calculated columns and measures.
B)Reports and Dashboards- PowerBi enables us to create reports and dashboards using the data.

6)Automation and Orchestration- 
ADF can be used to automate the entire process,from data ingestion to transformation and loading in PowerBi.We can triggers,schedule pipelines.This enables the flow of data from ingestion to visualization.

PROJECT 2 - (AZURE DATA FACTORY)

Requirement - 

1)we have azure blob storage account where we are getting raw data (employee,department)in Json format.

2)we need to create a pipeline and shedule on daily basis which will pic data from raw container to specefic employee and department container and we need to dump in CSV file.

3)We need to create pipeline which will be able to join the employee and department file based on department id and need to store into emp - department container.

4)We should able to save aggregrate information of total department wise employees salary and group by jobId.

**Design and Development ***-

Storage Blob account  has 4 container

1)raw container -> (employee.json,-> all the employee fill will go to employee container in csv
                   department.json) -> all the department file will go to department container in csv(create pipeline) step -1 & 2
				   
2)employee - employee.csv       join emp.csv and department.csv and store in emp -dept       container   (step -3) - creae Data Flow.
3)department- department.csv
4)emp-dept
5)aggregrate - Afte join it will do agregation sum of salary and group by jobId,Department Name.(step4)

Solution - 

1)Go to raw container and serch for file - employee.json, and department.json.
We have GetMetaData activity in Azure Dtaa FACTORY will give you all storage account information.

2)Use For ech activity - To iterate on multiple file  and If condition 
 If it is employee.json file then perform copy activity and  store in Employee container in csv format
 else if it is department.json file then perform copy activitystore in Department container in csv format
 
3)Use Dta Flow - once we get the csv file in employee and department container the will perform join , and store in emp-dept container.

4)Aggration logic - We will perform a aggregation logic and store in aggregrate container.
